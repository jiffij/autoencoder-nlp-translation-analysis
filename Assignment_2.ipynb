{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IM8rcgktVoHV"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h5JlZQn9H99",
        "outputId": "761a345b-9c7f-4f74-b7e9-b5df95470249"
      },
      "outputs": [],
      "source": [
        "# !pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmKXEj7rMI9E"
      },
      "source": [
        "Translation with a Sequence to Sequence Network\n",
        "*************************************************************\n",
        "\n",
        "::\n",
        "\n",
        "    [KEY: > input, = target, < output]\n",
        "\n",
        "    > il est en train de peindre un tableau .\n",
        "    = he is painting a picture .\n",
        "    < he is painting a picture .\n",
        "\n",
        "    > pourquoi ne pas essayer ce vin delicieux ?\n",
        "    = why not try that delicious wine ?\n",
        "    < why not try that delicious wine ?\n",
        "\n",
        "    > elle n est pas poete mais romanciere .\n",
        "    = she is not a poet but a novelist .\n",
        "    < she not not a poet but a novelist .\n",
        "\n",
        "    > vous etes trop maigre .\n",
        "    = you re too skinny .\n",
        "    < you re all alone .\n",
        "\n",
        "... to varying degrees of success.\n",
        "\n",
        "This is made possible by the simple but powerful idea of the `sequence\n",
        "to sequence network <http://arxiv.org/abs/1409.3215>`__, in which two\n",
        "recurrent neural networks work together to transform one sequence to\n",
        "another. An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
        "   :alt:\n",
        "\n",
        "To improve upon this model we'll use an `attention\n",
        "mechanism <https://arxiv.org/abs/1409.0473>`__, which lets the decoder\n",
        "learn to focus over a specific range of the input sequence.\n",
        "\n",
        "\n",
        "**Requirements**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrgMvmAeMI9J"
      },
      "source": [
        "Loading data files\n",
        "==================\n",
        "\n",
        "The data for this project is a set of many thousands of English to\n",
        "French translation pairs.\n",
        "\n",
        "`This question on Open Data Stack\n",
        "Exchange <http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages>`__\n",
        "pointed me to the open translation site http://tatoeba.org/ which has\n",
        "downloads available at http://tatoeba.org/eng/downloads - and better\n",
        "yet, someone did the extra work of splitting language pairs into\n",
        "individual text files here: http://www.manythings.org/anki/\n",
        "\n",
        "The English to French pairs are too big to include in the repo, so\n",
        "download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
        "separated list of translation pairs:\n",
        "\n",
        "::\n",
        "\n",
        "    I am cold.    J'ai froid.\n",
        "\n",
        ".. Note::\n",
        "   Download the data from\n",
        "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "   and extract it to the current directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUGpVT3MC9ZA",
        "outputId": "f9d3deac-733b-43e6-c297-2705a4a025e6"
      },
      "outputs": [],
      "source": [
        "# !wget http://www.manythings.org/anki/fra-eng.zip\n",
        "# !unzip -o fra-eng.zip\n",
        "# !mkdir data\n",
        "# !mv fra.txt data/eng-fra.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RZKd8K4BfBM1"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"<PAD>\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQpwb8giMI9S"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WtFJGPPytPV4"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49xLMyaYMI9X"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split\n",
        "lines into pairs. The files are all English → Other Language, so if we\n",
        "want to translate from Other Language → English I added the ``reverse``\n",
        "flag to reverse the pairs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AVZyk7glkhZ2"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QGX-Ffq6Iii"
      },
      "source": [
        "Since there are a *lot* of example sentences and we want to train\n",
        "something quickly, we'll trim the data set to only relatively short and\n",
        "simple sentences. Here the maximum length is 10 words (that includes\n",
        "ending punctuation) and we're filtering to sentences that translate to\n",
        "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
        "earlier).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8t1N5cYalfJS"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 15\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am\", \"i m\",\n",
        "    \"he is\", \"he s\",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re\",\n",
        "    \"we are\", \"we re\",\n",
        "    \"they are\", \"they re\"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A4Kf_ie6Jyr"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnGQl472lg0-",
        "outputId": "0f36edc9-7f7d-49ff-ed96-509a610f290b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 232736 sentence pairs\n",
            "Trimmed to 22907 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 7019\n",
            "eng 4638\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "# input_lang.addWord(\"<pad>\")\n",
        "# output_lang.addWord(\"<pad>\")\n",
        "#print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Tt6luSw3ym6k"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R50-9amYytmD"
      },
      "outputs": [],
      "source": [
        "X = [i[0] for i in pairs]\n",
        "y = [i[1] for i in pairs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ej3a3AvYyoI1"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X-69f4nmy5ph"
      },
      "outputs": [],
      "source": [
        "train_pairs = list(zip(X_train,y_train))\n",
        "test_pairs = list(zip(X_test,y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XxL347dMI9p"
      },
      "source": [
        "The Seq2Seq Model\n",
        "=================\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
        "seq2seq network, or `Encoder Decoder\n",
        "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
        "black cat\". Most of the words in the input sentence have a direct\n",
        "translation in the output sentence, but are in slightly different\n",
        "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
        "construction there is also one more word in the input sentence. It would\n",
        "be difficult to produce a correct translation directly from the sequence\n",
        "of input words.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YkUkdswMI9q"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "ust_lstm = False\n",
        "bidirectional = False\n",
        "attention = False\n",
        "useTransformer = False\n",
        "case = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O0aZQW30liTF"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bidirectional):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional=bidirectional)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        if self.bidirectional:\n",
        "            return (torch.zeros(2, 1, self.hidden_size, device=device), torch.zeros(2, 1, self.hidden_size, device=device))\n",
        "        else:\n",
        "            return (torch.zeros(1, 1, self.hidden_size, device=device), torch.zeros(1, 1, self.hidden_size, device=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 15):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = d_model\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        # self.out = nn.Linear()\n",
        "        self.linear = nn.Linear(d_model*MAX_LENGTH, d_model)\n",
        "        self.flatten = nn.Flatten(start_dim=0)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.linear.bias.data.zero_()\n",
        "        # self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            src: Tensor, shape ``[seq_len, batch_size]``\n",
        "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
        "        \"\"\"\n",
        "        src = self.embedding(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        \n",
        "        if False: #src_mask is None:\n",
        "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "            \"\"\"\n",
        "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = torch.mean(output, dim=0, keepdim=True)\n",
        "        # output = self.linear(self.flatten(output))\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvouslB-MI9t"
      },
      "source": [
        "The Decoder (Your assignment)\n",
        "-----------\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the translation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccvXiDVAMI9u"
      },
      "source": [
        "Simple Decoder\n",
        "^^^^^^^^^^^^^^\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "This last output is sometimes called the *context vector* as it encodes\n",
        "context from the entire sequence. This context vector is used as the\n",
        "initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and\n",
        "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "token, and the first hidden state is the context vector (the encoder's\n",
        "last hidden state).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DNY63vmP7pZT"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # Your code here #\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0])+1e-10)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding_size, output_size, max_length, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.embedding_size)\n",
        "        self.attn = nn.Linear(self.embedding_size + self.hidden_size, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.embedding_size + self.hidden_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        # print(attn_weights.size(), encoder_outputs.size())\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(AttDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        # Your code here #\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        # output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "    def concat_forward(self, input):\n",
        "        output = self.softmax(self.out(input))\n",
        "        return output\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        # Your code here #\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_size, device=device), torch.zeros(1, 1, self.hidden_size, device=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJTli8NHMI91"
      },
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
        "  limitation by using a relative position approach. Read about \"local\n",
        "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
        "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
        "\n",
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WrWHiE1RLtCz"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-s0fPrIMI99"
      },
      "source": [
        "Training the Model\n",
        "------------------\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but `when the trained\n",
        "network is exploited, it may exhibit\n",
        "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CvKmiwlnLtFf"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    if not useTransformer:\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "    if attention:\n",
        "        encoder_hidden_states = torch.zeros(max_length, 1, 1, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    if useTransformer:\n",
        "        # temp_input_tensor = torch.ones(max_length, dtype=torch.long, device=device)*2\n",
        "        # temp_output_tensor = torch.ones((max_length, 1), dtype=torch.long, device=device)*2\n",
        "        # temp_input_tensor[:input_length] = input_tensor.squeeze(1)\n",
        "        # temp_output_tensor[:target_length] = target_tensor\n",
        "        # input_tensor = temp_input_tensor\n",
        "        # target_tensor = temp_output_tensor\n",
        "        # input_length = input_tensor.size(0)\n",
        "        # target_length = target_tensor.size(0)\n",
        "        encoder_hidden = encoder(input_tensor)\n",
        "        # encoder_hidden = encoder_hidden.unsqueeze(0).unsqueeze(0)\n",
        "        \n",
        "        # encoder_hidden = torch.mean(encoder_hidden, dim=0).unsqueeze(0) # TODO\n",
        "        \n",
        "    else:\n",
        "        \n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(\n",
        "                input_tensor[ei], encoder_hidden)\n",
        "            # print(encoder_output[0, 0].size(), encoder_hidden[0].size())\n",
        "            encoder_outputs[ei] = encoder_output[0, 0] if not bidirectional else (encoder_output[0, 0][:hidden_size] + encoder_output[0, 0][hidden_size:])/2\n",
        "            if attention:\n",
        "                encoder_hidden_states[ei] = encoder_hidden\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    if not bidirectional:\n",
        "        decoder_hidden = encoder_hidden\n",
        "    else:\n",
        "        decoder_hidden = []\n",
        "        decoder_hidden.append(torch.mean(encoder_hidden[0], dim=0).unsqueeze(0))\n",
        "        decoder_hidden.append(torch.mean(encoder_hidden[1], dim=0).unsqueeze(0))\n",
        "        decoder_hidden = tuple(decoder_hidden)\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    softmax = nn.Softmax(0)\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            # decoder_output, decoder_hidden = decoder(\n",
        "            #     decoder_input, decoder_hidden)\n",
        "            if attention:\n",
        "                # cal attention score\n",
        "                attn_scr = torch.sum(encoder_hidden_states * torch.squeeze(decoder_hidden), dim=-1).squeeze()\n",
        "                attn_scr = softmax(attn_scr)\n",
        "                # print(attn_scr.size())\n",
        "                # attn_scr = torch.matmul(encoder_hidden_states, decoder_hidden)\n",
        "                attn_ws = torch.sum(encoder_hidden_states * attn_scr.view(-1, 1, 1, 1), dim=0)\n",
        "                decoder_hidden = (decoder_hidden + attn_ws)/2\n",
        "                # decoder_output = decoder.concat_forward(decoder_output).squeeze(0)\n",
        "            if case == 3:\n",
        "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            else:\n",
        "                decoder_output, decoder_hidden = decoder(\n",
        "                    decoder_input, decoder_hidden)\n",
        "            # print(decoder_output, target_tensor)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            if attention:\n",
        "                # cal attention score\n",
        "                attn_scr = torch.sum(encoder_hidden_states * torch.squeeze(decoder_hidden), dim=-1).squeeze()\n",
        "                attn_scr = softmax(attn_scr)\n",
        "                # print(attn_scr.size())\n",
        "                attn_ws = torch.sum(encoder_hidden_states * attn_scr.view(-1, 1, 1, 1), dim=0)   \n",
        "                decoder_hidden = (decoder_hidden + attn_ws)/2\n",
        "                # decoder_output = decoder.concat_forward(decoder_output).squeeze(0)\n",
        "            if case == 3:\n",
        "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            else:\n",
        "                decoder_output, decoder_hidden = decoder(\n",
        "                    decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            \n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            # if loss == torch.nan:\n",
        "            #     print(decoder_output, target_tensor)\n",
        "                \n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    # if not (loss / target_length) > 0:\n",
        "    #     print(loss, target_length)\n",
        "    \n",
        "    return loss / target_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxwlsYBxMI-A"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4Wb3PO24LwhG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu_oDmPEMI-D"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "# from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "rouge = ROUGEScore()\n",
        "\n",
        "def test(encoder, decoder, testing_pairs):\n",
        "    input = []\n",
        "    gt = []\n",
        "    predict = []\n",
        "    metric_score = {\n",
        "        \"rouge1_fmeasure\":[],\n",
        "        \"rouge1_precision\":[],\n",
        "        \"rouge1_recall\":[],\n",
        "        \"rouge2_fmeasure\":[],\n",
        "        \"rouge2_precision\":[],\n",
        "        \"rouge2_recall\":[]\n",
        "    }\n",
        "\n",
        "    for i in range(len(testing_pairs)):\n",
        "        pair = testing_pairs[i]\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "\n",
        "        input.append(pair[0])\n",
        "        gt.append(pair[1])\n",
        "        predict.append(output_sentence)\n",
        "\n",
        "        try:\n",
        "            rs = rouge(output_sentence, pair[1])\n",
        "        except:\n",
        "            continue\n",
        "        metric_score[\"rouge1_fmeasure\"].append(rs['rouge1_fmeasure'])\n",
        "        metric_score[\"rouge1_precision\"].append(rs['rouge1_precision'])\n",
        "        metric_score[\"rouge1_recall\"].append(rs['rouge1_recall'])\n",
        "        metric_score[\"rouge2_fmeasure\"].append(rs['rouge2_fmeasure'])\n",
        "        metric_score[\"rouge2_precision\"].append(rs['rouge2_precision'])\n",
        "        metric_score[\"rouge2_recall\"].append(rs['rouge2_recall'])\n",
        "\n",
        "    metric_score[\"rouge1_fmeasure\"] = np.array(metric_score[\"rouge1_fmeasure\"]).mean()\n",
        "    metric_score[\"rouge1_precision\"] = np.array(metric_score[\"rouge1_precision\"]).mean()\n",
        "    metric_score[\"rouge1_recall\"] = np.array(metric_score[\"rouge1_recall\"]).mean()\n",
        "    metric_score[\"rouge2_fmeasure\"] = np.array(metric_score[\"rouge2_fmeasure\"]).mean()\n",
        "    metric_score[\"rouge2_precision\"] = np.array(metric_score[\"rouge2_precision\"]).mean()\n",
        "    metric_score[\"rouge2_recall\"] = np.array(metric_score[\"rouge2_recall\"]).mean()\n",
        "\n",
        "    print(\"=== Evaluation score - Rouge score ===\")\n",
        "    print(\"Rouge1 fmeasure:\\t\",metric_score[\"rouge1_fmeasure\"])\n",
        "    print(\"Rouge1 precision:\\t\",metric_score[\"rouge1_precision\"])\n",
        "    print(\"Rouge1 recall:  \\t\",metric_score[\"rouge1_recall\"])\n",
        "    print(\"Rouge2 fmeasure:\\t\",metric_score[\"rouge2_fmeasure\"])\n",
        "    print(\"Rouge2 precision:\\t\",metric_score[\"rouge2_precision\"])\n",
        "    print(\"Rouge2 recall:  \\t\",metric_score[\"rouge2_recall\"])\n",
        "    print(\"=====================================\")\n",
        "    return input,gt,predict,metric_score\n",
        "\n",
        "# def deep_copy_model(model):\n",
        "#     # Create a new instance of the model\n",
        "#     # new_model = copy.deepcopy(model)\n",
        "    \n",
        "#     # Load the state_dict of the original model into the new model\n",
        "#     # new_model.load_state_dict(model.state_dict())\n",
        "    \n",
        "#     return new_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7mPu4xWzLx-O"
      },
      "outputs": [],
      "source": [
        "import pytorch_warmup as warmup\n",
        "\n",
        "def trainIters(encoder, decoder, epochs, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    best_score = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "    steps_per_epoch = len(train_pairs)\n",
        "    warmup_period = len(train_pairs)\n",
        "    num_steps = steps_per_epoch * epochs - warmup_period\n",
        "    t0 = num_steps // 3\n",
        "    lr_min = 1e-6\n",
        "    max_step = t0 * 3 + warmup_period\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    \n",
        "    e_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            encoder_optimizer, T_0=t0, T_mult=1, eta_min=lr_min)\n",
        "\n",
        "    e_warmup_scheduler = warmup.LinearWarmup(encoder_optimizer, warmup_period)\n",
        "    \n",
        "    d_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            decoder_optimizer, T_0=t0, T_mult=1, eta_min=lr_min)\n",
        "\n",
        "    d_warmup_scheduler = warmup.LinearWarmup(decoder_optimizer, warmup_period)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    iter = 1\n",
        "    n_iters = len(train_pairs) * epochs\n",
        "    \n",
        "    best_encoder_state = None\n",
        "    best_decoder_state = None\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        print(\"Epoch: %d/%d\" % (epoch, epochs))\n",
        "        # old_encoder = deep_copy_model(encoder)\n",
        "        # old_decoder = deep_copy_model(decoder)\n",
        "        for training_pair in train_pairs:\n",
        "            training_pair = tensorsFromPair(training_pair)\n",
        "\n",
        "            input_tensor = training_pair[0]\n",
        "            target_tensor = training_pair[1]\n",
        "\n",
        "            loss = train(input_tensor, target_tensor, encoder,\n",
        "                        decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "            print_loss_total += loss\n",
        "            plot_loss_total += loss\n",
        "            \n",
        "            with e_warmup_scheduler.dampening():\n",
        "                if e_warmup_scheduler.last_step + 1 >= warmup_period:\n",
        "                    e_lr_scheduler.step()\n",
        "            # if e_warmup_scheduler.last_step + 1 >= max_step:\n",
        "            #     break\n",
        "            \n",
        "            with d_warmup_scheduler.dampening():\n",
        "                if d_warmup_scheduler.last_step + 1 >= warmup_period:\n",
        "                    d_lr_scheduler.step()\n",
        "            # if d_warmup_scheduler.last_step + 1 >= max_step:\n",
        "            #     break\n",
        "\n",
        "            if iter % print_every == 0:\n",
        "                print_loss_avg = print_loss_total / print_every\n",
        "                print_loss_total = 0\n",
        "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                            iter, iter / n_iters * 100, print_loss_avg))\n",
        "                plot_losses.append(print_loss_avg)\n",
        "\n",
        "            iter +=1\n",
        "        input,gt,predict,score = test(encoder, decoder, test_pairs)\n",
        "        if np.mean(np.array([score['rouge1_fmeasure'], score['rouge2_fmeasure']])) <= best_score: #and (epoch - best_epoch) > 10:\n",
        "            # break\n",
        "            pass\n",
        "        else:\n",
        "            # best_encoder_state = encoder.state_dict()\n",
        "            # best_decoder_state = decoder.state_dict()\n",
        "            torch.save(encoder.state_dict(), 'encoder_weights.pth')\n",
        "            torch.save(decoder.state_dict(), 'decoder_weights.pth')\n",
        "            best_score = np.mean(np.array([score['rouge1_fmeasure'], score['rouge2_fmeasure']]))\n",
        "    encoder.load_state_dict(torch.load('encoder_weights.pth'))\n",
        "    decoder.load_state_dict(torch.load('decoder_weights.pth'))\n",
        "    return plot_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s52RCR0MI-K"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tCrTcoH0L04g"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        if not useTransformer:\n",
        "            encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "        if attention:\n",
        "            encoder_hidden_states = torch.zeros(max_length, 1, 1, encoder.hidden_size, device=device)\n",
        "\n",
        "        if useTransformer:\n",
        "            encoder_hidden = encoder(input_tensor)\n",
        "            encoder_hidden = torch.mean(encoder_hidden, dim=0).unsqueeze(0)\n",
        "        else:\n",
        "            for ei in range(input_length):\n",
        "                encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                        encoder_hidden)\n",
        "                if attention:\n",
        "                    encoder_hidden_states[ei] = encoder_hidden\n",
        "                encoder_outputs[ei] += encoder_output[0, 0] if not bidirectional else (encoder_output[0, 0][:hidden_size] + encoder_output[0, 0][hidden_size:])/2\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        if not bidirectional:\n",
        "            decoder_hidden = encoder_hidden\n",
        "        else:\n",
        "            decoder_hidden = []\n",
        "            decoder_hidden.append(torch.mean(encoder_hidden[0], dim=0).unsqueeze(0))\n",
        "            decoder_hidden.append(torch.mean(encoder_hidden[1], dim=0).unsqueeze(0))\n",
        "            decoder_hidden = tuple(decoder_hidden)\n",
        "\n",
        "        decoded_words = []\n",
        "        softmax = nn.Softmax(0)\n",
        "        for di in range(max_length):\n",
        "            if attention:\n",
        "                # cal attention score\n",
        "                attn_scr = torch.sum(encoder_hidden_states * torch.squeeze(decoder_hidden), dim=-1).squeeze()\n",
        "                # attn_scr = torch.matmul(encoder_hidden_states, decoder_hidden)\n",
        "                attn_scr = softmax(attn_scr)\n",
        "                attn_ws = torch.mean(encoder_hidden_states * attn_scr.view(-1, 1, 1, 1), dim=0)   \n",
        "                decoder_hidden = (decoder_hidden + attn_ws)/2\n",
        "                # decoder_output = decoder.concat_forward(decoder_output)\n",
        "            if case == 3:\n",
        "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            else:\n",
        "                decoder_output, decoder_hidden = decoder(\n",
        "                    decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DXbiFX1MI-N"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "XoYOvTaOL4oI"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1DWJXsfd2q4A"
      },
      "outputs": [],
      "source": [
        "# from torchmetrics.text.rouge import ROUGEScore\n",
        "# # from tqdm import tqdm\n",
        "# import numpy as np\n",
        "\n",
        "# rouge = ROUGEScore()\n",
        "\n",
        "# def test(encoder, decoder, testing_pairs):\n",
        "#     input = []\n",
        "#     gt = []\n",
        "#     predict = []\n",
        "#     metric_score = {\n",
        "#         \"rouge1_fmeasure\":[],\n",
        "#         \"rouge1_precision\":[],\n",
        "#         \"rouge1_recall\":[],\n",
        "#         \"rouge2_fmeasure\":[],\n",
        "#         \"rouge2_precision\":[],\n",
        "#         \"rouge2_recall\":[]\n",
        "#     }\n",
        "\n",
        "#     for i in range(len(testing_pairs)):\n",
        "#         pair = testing_pairs[i]\n",
        "#         output_words = evaluate(encoder, decoder, pair[0])\n",
        "#         output_sentence = ' '.join(output_words)\n",
        "\n",
        "#         input.append(pair[0])\n",
        "#         gt.append(pair[1])\n",
        "#         predict.append(output_sentence)\n",
        "\n",
        "#         try:\n",
        "#             rs = rouge(output_sentence, pair[1])\n",
        "#         except:\n",
        "#             continue\n",
        "#         metric_score[\"rouge1_fmeasure\"].append(rs['rouge1_fmeasure'])\n",
        "#         metric_score[\"rouge1_precision\"].append(rs['rouge1_precision'])\n",
        "#         metric_score[\"rouge1_recall\"].append(rs['rouge1_recall'])\n",
        "#         metric_score[\"rouge2_fmeasure\"].append(rs['rouge2_fmeasure'])\n",
        "#         metric_score[\"rouge2_precision\"].append(rs['rouge2_precision'])\n",
        "#         metric_score[\"rouge2_recall\"].append(rs['rouge2_recall'])\n",
        "\n",
        "#     metric_score[\"rouge1_fmeasure\"] = np.array(metric_score[\"rouge1_fmeasure\"]).mean()\n",
        "#     metric_score[\"rouge1_precision\"] = np.array(metric_score[\"rouge1_precision\"]).mean()\n",
        "#     metric_score[\"rouge1_recall\"] = np.array(metric_score[\"rouge1_recall\"]).mean()\n",
        "#     metric_score[\"rouge2_fmeasure\"] = np.array(metric_score[\"rouge2_fmeasure\"]).mean()\n",
        "#     metric_score[\"rouge2_precision\"] = np.array(metric_score[\"rouge2_precision\"]).mean()\n",
        "#     metric_score[\"rouge2_recall\"] = np.array(metric_score[\"rouge2_recall\"]).mean()\n",
        "\n",
        "#     print(\"=== Evaluation score - Rouge score ===\")\n",
        "#     print(\"Rouge1 fmeasure:\\t\",metric_score[\"rouge1_fmeasure\"])\n",
        "#     print(\"Rouge1 precision:\\t\",metric_score[\"rouge1_precision\"])\n",
        "#     print(\"Rouge1 recall:  \\t\",metric_score[\"rouge1_recall\"])\n",
        "#     print(\"Rouge2 fmeasure:\\t\",metric_score[\"rouge2_fmeasure\"])\n",
        "#     print(\"Rouge2 precision:\\t\",metric_score[\"rouge2_precision\"])\n",
        "#     print(\"Rouge2 recall:  \\t\",metric_score[\"rouge2_recall\"])\n",
        "#     print(\"=====================================\")\n",
        "#     return input,gt,predict,metric_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppy3qh7fMI-R"
      },
      "source": [
        "Training and Evaluating\n",
        "=======================\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot(x):\n",
        "    plt.plot(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzMeJbVDL6Un",
        "outputId": "dab447ba-4af6-4dec-d98a-af287fa9851b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/50\n",
            "0m 13s (- 223m 14s) (1000 0%) 6.2979\n",
            "0m 25s (- 218m 29s) (2000 0%) 3.8931\n",
            "0m 39s (- 223m 22s) (3000 0%) 3.6254\n",
            "0m 51s (- 222m 6s) (4000 0%) 3.6650\n",
            "1m 5s (- 224m 9s) (5000 0%) 3.6398\n",
            "1m 18s (- 224m 0s) (6000 0%) 3.5854\n",
            "1m 31s (- 223m 38s) (7000 0%) 3.5185\n",
            "1m 44s (- 223m 39s) (8000 0%) 3.5175\n",
            "1m 58s (- 224m 4s) (9000 0%) 3.5059\n",
            "2m 11s (- 224m 7s) (10000 0%) 3.4548\n",
            "2m 25s (- 224m 50s) (11000 1%) 3.3743\n",
            "2m 39s (- 226m 4s) (12000 1%) 3.3585\n",
            "2m 54s (- 227m 26s) (13000 1%) 3.2413\n",
            "3m 8s (- 227m 37s) (14000 1%) 3.1713\n",
            "3m 22s (- 228m 7s) (15000 1%) 3.1321\n",
            "3m 35s (- 227m 43s) (16000 1%) 3.1184\n",
            "3m 49s (- 227m 42s) (17000 1%) 3.1027\n",
            "4m 3s (- 228m 24s) (18000 1%) 3.0910\n",
            "4m 18s (- 229m 30s) (19000 1%) 3.1036\n",
            "4m 32s (- 229m 55s) (20000 1%) 3.0247\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.37897098\n",
            "Rouge1 precision:\t 0.39310345\n",
            "Rouge1 recall:  \t 0.37705496\n",
            "Rouge2 fmeasure:\t 0.19791102\n",
            "Rouge2 precision:\t 0.20158488\n",
            "Rouge2 recall:  \t 0.20355804\n",
            "=====================================\n",
            "Epoch: 2/50\n",
            "6m 9s (- 295m 55s) (21000 2%) 3.0488\n",
            "6m 36s (- 303m 13s) (22000 2%) 3.0495\n",
            "6m 50s (- 299m 38s) (23000 2%) 2.9977\n",
            "7m 3s (- 296m 25s) (24000 2%) 2.9791\n",
            "7m 17s (- 293m 7s) (25000 2%) 2.9232\n",
            "7m 42s (- 297m 51s) (26000 2%) 2.9792\n",
            "8m 9s (- 303m 27s) (27000 2%) 2.9299\n",
            "8m 24s (- 300m 52s) (28000 2%) 2.8610\n",
            "8m 38s (- 298m 22s) (29000 2%) 2.8967\n",
            "8m 52s (- 295m 56s) (30000 2%) 2.8638\n",
            "9m 5s (- 293m 18s) (31000 3%) 2.8491\n",
            "9m 19s (- 290m 52s) (32000 3%) 2.8492\n",
            "9m 32s (- 288m 40s) (33000 3%) 2.8923\n",
            "9m 46s (- 286m 36s) (34000 3%) 2.8692\n",
            "10m 1s (- 285m 2s) (35000 3%) 2.8090\n",
            "10m 15s (- 283m 28s) (36000 3%) 2.8169\n",
            "10m 30s (- 282m 2s) (37000 3%) 2.8148\n",
            "10m 44s (- 280m 31s) (38000 3%) 2.8075\n",
            "10m 58s (- 279m 8s) (39000 3%) 2.8369\n",
            "11m 12s (- 277m 33s) (40000 3%) 2.7279\n",
            "11m 35s (- 279m 53s) (41000 3%) 2.7745\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.3859736\n",
            "Rouge1 precision:\t 0.3876835\n",
            "Rouge1 recall:  \t 0.39635122\n",
            "Rouge2 fmeasure:\t 0.21281157\n",
            "Rouge2 precision:\t 0.21153219\n",
            "Rouge2 recall:  \t 0.22468354\n",
            "=====================================\n",
            "Epoch: 3/50\n",
            "12m 7s (- 285m 22s) (42000 4%) 2.7592\n",
            "12m 20s (- 283m 28s) (43000 4%) 2.7569\n",
            "12m 33s (- 281m 46s) (44000 4%) 2.6968\n",
            "12m 47s (- 280m 15s) (45000 4%) 2.7482\n",
            "13m 1s (- 278m 48s) (46000 4%) 2.7031\n",
            "13m 15s (- 277m 23s) (47000 4%) 2.7364\n",
            "13m 28s (- 275m 58s) (48000 4%) 2.6759\n",
            "13m 42s (- 274m 36s) (49000 4%) 2.6840\n",
            "13m 55s (- 273m 14s) (50000 4%) 2.7094\n",
            "14m 9s (- 272m 3s) (51000 4%) 2.6300\n",
            "14m 23s (- 270m 51s) (52000 5%) 2.6372\n",
            "14m 37s (- 269m 49s) (53000 5%) 2.6610\n",
            "14m 51s (- 268m 52s) (54000 5%) 2.6525\n",
            "15m 10s (- 269m 19s) (55000 5%) 2.6588\n",
            "15m 53s (- 276m 44s) (56000 5%) 2.6544\n",
            "16m 36s (- 283m 52s) (57000 5%) 2.5855\n",
            "16m 57s (- 284m 20s) (58000 5%) 2.6185\n",
            "17m 24s (- 286m 46s) (59000 5%) 2.5981\n",
            "17m 38s (- 285m 23s) (60000 5%) 2.5885\n",
            "17m 58s (- 285m 46s) (61000 5%) 2.5139\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.41200453\n",
            "Rouge1 precision:\t 0.4076247\n",
            "Rouge1 recall:  \t 0.42774415\n",
            "Rouge2 fmeasure:\t 0.2297131\n",
            "Rouge2 precision:\t 0.22359909\n",
            "Rouge2 recall:  \t 0.24611586\n",
            "=====================================\n",
            "Epoch: 4/50\n",
            "18m 39s (- 291m 29s) (62000 6%) 2.5766\n",
            "18m 58s (- 291m 23s) (63000 6%) 2.5994\n",
            "19m 11s (- 289m 54s) (64000 6%) 2.5741\n",
            "19m 25s (- 288m 34s) (65000 6%) 2.5274\n",
            "19m 38s (- 287m 10s) (66000 6%) 2.5552\n",
            "19m 52s (- 285m 51s) (67000 6%) 2.5639\n",
            "20m 6s (- 284m 38s) (68000 6%) 2.5352\n",
            "20m 20s (- 283m 28s) (69000 6%) 2.4880\n",
            "20m 34s (- 282m 19s) (70000 6%) 2.5152\n",
            "20m 48s (- 281m 13s) (71000 6%) 2.5082\n",
            "21m 2s (- 280m 14s) (72000 6%) 2.4990\n",
            "21m 16s (- 279m 4s) (73000 7%) 2.5037\n",
            "21m 38s (- 279m 44s) (74000 7%) 2.5072\n",
            "21m 52s (- 278m 44s) (75000 7%) 2.5137\n",
            "22m 6s (- 277m 47s) (76000 7%) 2.4746\n",
            "22m 33s (- 279m 24s) (77000 7%) 2.4648\n",
            "22m 57s (- 280m 25s) (78000 7%) 2.4458\n",
            "23m 40s (- 285m 18s) (79000 7%) 2.4533\n",
            "24m 23s (- 289m 59s) (80000 7%) 2.4996\n",
            "25m 7s (- 294m 32s) (81000 7%) 2.4147\n",
            "25m 46s (- 298m 15s) (82000 7%) 2.4354\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.43337455\n",
            "Rouge1 precision:\t 0.43555602\n",
            "Rouge1 recall:  \t 0.4419054\n",
            "Rouge2 fmeasure:\t 0.251004\n",
            "Rouge2 precision:\t 0.24936919\n",
            "Rouge2 recall:  \t 0.26235968\n",
            "=====================================\n",
            "Epoch: 5/50\n",
            "26m 53s (- 307m 7s) (83000 8%) 2.4301\n",
            "27m 7s (- 305m 46s) (84000 8%) 2.4688\n",
            "27m 22s (- 304m 30s) (85000 8%) 2.3594\n",
            "28m 7s (- 308m 59s) (86000 8%) 2.4380\n",
            "28m 39s (- 310m 57s) (87000 8%) 2.3841\n",
            "28m 54s (- 309m 38s) (88000 8%) 2.3723\n",
            "29m 8s (- 308m 19s) (89000 8%) 2.4105\n",
            "29m 22s (- 307m 2s) (90000 8%) 2.3575\n",
            "29m 36s (- 305m 46s) (91000 8%) 2.3650\n",
            "29m 50s (- 304m 31s) (92000 8%) 2.3756\n",
            "30m 4s (- 303m 14s) (93000 9%) 2.3147\n",
            "30m 45s (- 306m 32s) (94000 9%) 2.3442\n",
            "30m 59s (- 305m 20s) (95000 9%) 2.3618\n",
            "31m 13s (- 304m 0s) (96000 9%) 2.3039\n",
            "31m 33s (- 303m 44s) (97000 9%) 2.3588\n",
            "31m 46s (- 302m 27s) (98000 9%) 2.3155\n",
            "32m 0s (- 301m 16s) (99000 9%) 2.3182\n",
            "32m 15s (- 300m 12s) (100000 9%) 2.3118\n",
            "32m 29s (- 299m 3s) (101000 9%) 2.3944\n",
            "32m 43s (- 297m 58s) (102000 9%) 2.2808\n",
            "33m 9s (- 298m 40s) (103000 9%) 2.2891\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.425118\n",
            "Rouge1 precision:\t 0.41373327\n",
            "Rouge1 recall:  \t 0.4469747\n",
            "Rouge2 fmeasure:\t 0.24797153\n",
            "Rouge2 precision:\t 0.23811543\n",
            "Rouge2 recall:  \t 0.2677185\n",
            "=====================================\n",
            "Epoch: 6/50\n",
            "34m 50s (- 310m 30s) (104000 10%) 2.3140\n",
            "35m 4s (- 309m 17s) (105000 10%) 2.3077\n",
            "35m 18s (- 308m 5s) (106000 10%) 2.2648\n",
            "35m 32s (- 306m 54s) (107000 10%) 2.3164\n",
            "35m 46s (- 305m 42s) (108000 10%) 2.2721\n",
            "36m 0s (- 304m 32s) (109000 10%) 2.2796\n",
            "36m 16s (- 303m 39s) (110000 10%) 2.2570\n",
            "36m 33s (- 302m 58s) (111000 10%) 2.2336\n",
            "36m 47s (- 301m 50s) (112000 10%) 2.2330\n",
            "37m 1s (- 300m 46s) (113000 10%) 2.2026\n",
            "37m 15s (- 299m 40s) (114000 11%) 2.2297\n",
            "37m 29s (- 298m 31s) (115000 11%) 2.2439\n",
            "37m 43s (- 297m 27s) (116000 11%) 2.2393\n",
            "37m 57s (- 296m 28s) (117000 11%) 2.2015\n",
            "38m 12s (- 295m 32s) (118000 11%) 2.1850\n",
            "38m 26s (- 294m 31s) (119000 11%) 2.2246\n",
            "38m 40s (- 293m 30s) (120000 11%) 2.2102\n",
            "38m 54s (- 292m 33s) (121000 11%) 2.2469\n",
            "39m 8s (- 291m 32s) (122000 11%) 2.2057\n",
            "39m 22s (- 290m 34s) (123000 11%) 2.1614\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.4314878\n",
            "Rouge1 precision:\t 0.41649947\n",
            "Rouge1 recall:  \t 0.45738536\n",
            "Rouge2 fmeasure:\t 0.2551446\n",
            "Rouge2 precision:\t 0.24299425\n",
            "Rouge2 recall:  \t 0.27748868\n",
            "=====================================\n",
            "Epoch: 7/50\n",
            "39m 54s (- 291m 50s) (124000 12%) 2.1851\n",
            "40m 9s (- 290m 58s) (125000 12%) 2.2549\n",
            "40m 23s (- 290m 6s) (126000 12%) 2.1877\n",
            "40m 38s (- 289m 15s) (127000 12%) 2.1800\n",
            "40m 53s (- 288m 23s) (128000 12%) 2.1568\n",
            "41m 7s (- 287m 27s) (129000 12%) 2.1610\n",
            "41m 20s (- 286m 27s) (130000 12%) 2.1684\n",
            "41m 34s (- 285m 32s) (131000 12%) 2.1007\n",
            "41m 48s (- 284m 38s) (132000 12%) 2.0958\n",
            "42m 1s (- 283m 40s) (133000 12%) 2.0991\n",
            "42m 14s (- 282m 45s) (134000 12%) 2.0923\n",
            "42m 28s (- 281m 49s) (135000 13%) 2.1044\n",
            "42m 42s (- 280m 58s) (136000 13%) 2.1055\n",
            "42m 56s (- 280m 6s) (137000 13%) 2.0984\n",
            "43m 9s (- 279m 15s) (138000 13%) 2.0723\n",
            "43m 23s (- 278m 21s) (139000 13%) 2.1051\n",
            "43m 37s (- 277m 32s) (140000 13%) 2.1039\n",
            "43m 50s (- 276m 40s) (141000 13%) 2.1073\n",
            "44m 4s (- 275m 54s) (142000 13%) 2.1145\n",
            "44m 18s (- 275m 7s) (143000 13%) 2.0464\n",
            "44m 33s (- 274m 21s) (144000 13%) 2.0580\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.43629286\n",
            "Rouge1 precision:\t 0.41826376\n",
            "Rouge1 recall:  \t 0.46550772\n",
            "Rouge2 fmeasure:\t 0.26119775\n",
            "Rouge2 precision:\t 0.24643199\n",
            "Rouge2 recall:  \t 0.28661367\n",
            "=====================================\n",
            "Epoch: 8/50\n",
            "45m 5s (- 275m 29s) (145000 14%) 2.0825\n",
            "45m 19s (- 274m 40s) (146000 14%) 2.0430\n",
            "45m 33s (- 273m 54s) (147000 14%) 2.0015\n",
            "45m 57s (- 274m 10s) (148000 14%) 2.0913\n",
            "46m 12s (- 273m 27s) (149000 14%) 2.0240\n",
            "46m 39s (- 273m 57s) (150000 14%) 2.0463\n",
            "46m 54s (- 273m 16s) (151000 14%) 2.0156\n",
            "47m 7s (- 272m 27s) (152000 14%) 1.9996\n",
            "47m 21s (- 271m 42s) (153000 14%) 2.0134\n",
            "47m 35s (- 270m 55s) (154000 14%) 1.9685\n",
            "47m 48s (- 270m 10s) (155000 15%) 2.0260\n",
            "48m 2s (- 269m 22s) (156000 15%) 1.9526\n",
            "48m 15s (- 268m 34s) (157000 15%) 1.9643\n",
            "48m 29s (- 267m 50s) (158000 15%) 2.0003\n",
            "48m 42s (- 267m 5s) (159000 15%) 2.0052\n",
            "48m 56s (- 266m 20s) (160000 15%) 1.9768\n",
            "49m 10s (- 265m 38s) (161000 15%) 1.9562\n",
            "49m 23s (- 264m 54s) (162000 15%) 1.9410\n",
            "49m 37s (- 264m 14s) (163000 15%) 2.0255\n",
            "49m 51s (- 263m 31s) (164000 15%) 1.9412\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.44397715\n",
            "Rouge1 precision:\t 0.43174478\n",
            "Rouge1 recall:  \t 0.46727353\n",
            "Rouge2 fmeasure:\t 0.26536095\n",
            "Rouge2 precision:\t 0.25401756\n",
            "Rouge2 recall:  \t 0.28687865\n",
            "=====================================\n",
            "Epoch: 9/50\n",
            "50m 23s (- 264m 26s) (165000 16%) 1.9931\n",
            "50m 44s (- 264m 21s) (166000 16%) 1.9966\n",
            "51m 3s (- 264m 6s) (167000 16%) 1.9399\n",
            "51m 17s (- 263m 25s) (168000 16%) 1.8777\n",
            "51m 31s (- 262m 44s) (169000 16%) 1.9961\n",
            "51m 45s (- 262m 4s) (170000 16%) 1.9117\n",
            "51m 58s (- 261m 21s) (171000 16%) 1.9637\n",
            "52m 13s (- 260m 46s) (172000 16%) 1.8719\n",
            "52m 27s (- 260m 8s) (173000 16%) 1.8929\n",
            "52m 42s (- 259m 31s) (174000 16%) 1.8555\n",
            "52m 56s (- 258m 53s) (175000 16%) 1.8420\n",
            "53m 10s (- 258m 14s) (176000 17%) 1.9116\n",
            "53m 23s (- 257m 34s) (177000 17%) 1.8618\n",
            "53m 37s (- 256m 57s) (178000 17%) 1.8703\n",
            "53m 51s (- 256m 19s) (179000 17%) 1.8603\n",
            "54m 5s (- 255m 40s) (180000 17%) 1.8305\n",
            "54m 19s (- 255m 5s) (181000 17%) 1.8729\n",
            "54m 33s (- 254m 27s) (182000 17%) 1.8450\n",
            "54m 47s (- 253m 51s) (183000 17%) 1.8956\n",
            "55m 1s (- 253m 15s) (184000 17%) 1.8440\n",
            "55m 15s (- 252m 38s) (185000 17%) 1.7857\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.4607299\n",
            "Rouge1 precision:\t 0.440607\n",
            "Rouge1 recall:  \t 0.49250442\n",
            "Rouge2 fmeasure:\t 0.28829032\n",
            "Rouge2 precision:\t 0.2712762\n",
            "Rouge2 recall:  \t 0.31655964\n",
            "=====================================\n",
            "Epoch: 10/50\n",
            "55m 48s (- 253m 28s) (186000 18%) 1.8236\n",
            "56m 3s (- 252m 54s) (187000 18%) 1.8884\n",
            "56m 17s (- 252m 20s) (188000 18%) 1.7906\n",
            "56m 31s (- 251m 45s) (189000 18%) 1.8051\n",
            "56m 45s (- 251m 11s) (190000 18%) 1.7984\n",
            "57m 0s (- 250m 38s) (191000 18%) 1.7814\n",
            "57m 35s (- 251m 36s) (192000 18%) 1.8281\n",
            "57m 54s (- 251m 20s) (193000 18%) 1.7715\n",
            "58m 9s (- 250m 50s) (194000 18%) 1.7687\n",
            "58m 45s (- 251m 50s) (195000 18%) 1.7463\n",
            "59m 19s (- 252m 39s) (196000 19%) 1.7666\n",
            "59m 33s (- 252m 5s) (197000 19%) 1.7583\n",
            "59m 47s (- 251m 30s) (198000 19%) 1.7632\n",
            "60m 2s (- 250m 56s) (199000 19%) 1.7395\n",
            "60m 15s (- 250m 20s) (200000 19%) 1.7529\n",
            "60m 30s (- 249m 48s) (201000 19%) 1.7329\n",
            "60m 44s (- 249m 12s) (202000 19%) 1.6776\n",
            "60m 58s (- 248m 38s) (203000 19%) 1.7503\n",
            "61m 12s (- 248m 4s) (204000 19%) 1.7412\n",
            "61m 26s (- 247m 29s) (205000 19%) 1.6982\n",
            "61m 40s (- 246m 56s) (206000 19%) 1.7284\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.4735616\n",
            "Rouge1 precision:\t 0.4558696\n",
            "Rouge1 recall:  \t 0.5027378\n",
            "Rouge2 fmeasure:\t 0.3005517\n",
            "Rouge2 precision:\t 0.28536496\n",
            "Rouge2 recall:  \t 0.32674995\n",
            "=====================================\n",
            "Epoch: 11/50\n",
            "62m 12s (- 247m 33s) (207000 20%) 1.6850\n",
            "62m 26s (- 247m 1s) (208000 20%) 1.7538\n",
            "62m 40s (- 246m 28s) (209000 20%) 1.6670\n",
            "62m 55s (- 245m 55s) (210000 20%) 1.7270\n",
            "63m 9s (- 245m 21s) (211000 20%) 1.6909\n",
            "63m 23s (- 244m 48s) (212000 20%) 1.6784\n",
            "63m 37s (- 244m 15s) (213000 20%) 1.6878\n",
            "63m 51s (- 243m 42s) (214000 20%) 1.6959\n",
            "64m 6s (- 243m 13s) (215000 20%) 1.6692\n",
            "64m 20s (- 242m 44s) (216000 20%) 1.5980\n",
            "64m 36s (- 242m 17s) (217000 21%) 1.6492\n",
            "64m 51s (- 241m 49s) (218000 21%) 1.6082\n",
            "65m 5s (- 241m 16s) (219000 21%) 1.5980\n",
            "65m 19s (- 240m 44s) (220000 21%) 1.5800\n",
            "65m 33s (- 240m 13s) (221000 21%) 1.6780\n",
            "65m 47s (- 239m 41s) (222000 21%) 1.6014\n",
            "66m 1s (- 239m 9s) (223000 21%) 1.6176\n",
            "66m 14s (- 238m 36s) (224000 21%) 1.6479\n",
            "66m 28s (- 238m 5s) (225000 21%) 1.5881\n",
            "66m 43s (- 237m 35s) (226000 21%) 1.5811\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.4843623\n",
            "Rouge1 precision:\t 0.46151274\n",
            "Rouge1 recall:  \t 0.518888\n",
            "Rouge2 fmeasure:\t 0.31407863\n",
            "Rouge2 precision:\t 0.29489934\n",
            "Rouge2 recall:  \t 0.34465766\n",
            "=====================================\n",
            "Epoch: 12/50\n",
            "67m 16s (- 238m 11s) (227000 22%) 1.5727\n",
            "67m 30s (- 237m 40s) (228000 22%) 1.5974\n",
            "67m 44s (- 237m 10s) (229000 22%) 1.5964\n",
            "67m 58s (- 236m 40s) (230000 22%) 1.5568\n",
            "68m 12s (- 236m 9s) (231000 22%) 1.5917\n",
            "68m 27s (- 235m 41s) (232000 22%) 1.5509\n",
            "68m 41s (- 235m 13s) (233000 22%) 1.5942\n",
            "68m 56s (- 234m 44s) (234000 22%) 1.5437\n",
            "69m 10s (- 234m 14s) (235000 22%) 1.5607\n",
            "69m 24s (- 233m 45s) (236000 22%) 1.5035\n",
            "69m 38s (- 233m 15s) (237000 22%) 1.5098\n",
            "70m 17s (- 234m 7s) (238000 23%) 1.5566\n",
            "70m 56s (- 235m 1s) (239000 23%) 1.5288\n",
            "71m 35s (- 235m 52s) (240000 23%) 1.5278\n",
            "72m 13s (- 236m 42s) (241000 23%) 1.4893\n",
            "72m 33s (- 236m 30s) (242000 23%) 1.5027\n",
            "72m 53s (- 236m 18s) (243000 23%) 1.5043\n",
            "73m 6s (- 235m 45s) (244000 23%) 1.5416\n",
            "73m 21s (- 235m 15s) (245000 23%) 1.5319\n",
            "73m 34s (- 234m 44s) (246000 23%) 1.4835\n",
            "73m 49s (- 234m 14s) (247000 23%) 1.4838\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.49801332\n",
            "Rouge1 precision:\t 0.47272158\n",
            "Rouge1 recall:  \t 0.53529286\n",
            "Rouge2 fmeasure:\t 0.32820976\n",
            "Rouge2 precision:\t 0.30668113\n",
            "Rouge2 recall:  \t 0.36150482\n",
            "=====================================\n",
            "Epoch: 13/50\n",
            "74m 28s (- 235m 6s) (248000 24%) 1.4759\n",
            "74m 44s (- 234m 41s) (249000 24%) 1.5481\n",
            "75m 29s (- 235m 46s) (250000 24%) 1.4257\n",
            "76m 13s (- 236m 49s) (251000 24%) 1.5217\n",
            "76m 53s (- 237m 38s) (252000 24%) 1.4768\n",
            "77m 7s (- 237m 7s) (253000 24%) 1.4604\n",
            "77m 22s (- 236m 37s) (254000 24%) 1.5264\n",
            "77m 36s (- 236m 5s) (255000 24%) 1.4486\n",
            "77m 50s (- 235m 35s) (256000 24%) 1.4761\n",
            "78m 4s (- 235m 5s) (257000 24%) 1.3986\n",
            "78m 18s (- 234m 33s) (258000 25%) 1.4212\n",
            "78m 32s (- 234m 2s) (259000 25%) 1.4189\n",
            "78m 46s (- 233m 33s) (260000 25%) 1.4201\n",
            "79m 1s (- 233m 3s) (261000 25%) 1.4077\n",
            "79m 15s (- 232m 33s) (262000 25%) 1.3968\n",
            "79m 29s (- 232m 3s) (263000 25%) 1.3961\n",
            "79m 50s (- 231m 53s) (264000 25%) 1.3729\n",
            "80m 4s (- 231m 24s) (265000 25%) 1.4219\n",
            "80m 19s (- 230m 56s) (266000 25%) 1.4266\n",
            "80m 34s (- 230m 28s) (267000 25%) 1.4148\n",
            "80m 48s (- 230m 1s) (268000 25%) 1.3763\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.50987077\n",
            "Rouge1 precision:\t 0.48601046\n",
            "Rouge1 recall:  \t 0.5454726\n",
            "Rouge2 fmeasure:\t 0.33927345\n",
            "Rouge2 precision:\t 0.31794447\n",
            "Rouge2 recall:  \t 0.37251878\n",
            "=====================================\n",
            "Epoch: 14/50\n",
            "81m 22s (- 230m 25s) (269000 26%) 1.4251\n",
            "81m 36s (- 229m 56s) (270000 26%) 1.4217\n",
            "81m 50s (- 229m 26s) (271000 26%) 1.3291\n",
            "82m 4s (- 228m 58s) (272000 26%) 1.4476\n",
            "82m 19s (- 228m 29s) (273000 26%) 1.4019\n",
            "82m 33s (- 228m 2s) (274000 26%) 1.3456\n",
            "82m 48s (- 227m 35s) (275000 26%) 1.3664\n",
            "83m 3s (- 227m 7s) (276000 26%) 1.3559\n",
            "83m 17s (- 226m 40s) (277000 26%) 1.3533\n",
            "83m 32s (- 226m 13s) (278000 26%) 1.3106\n",
            "83m 50s (- 225m 55s) (279000 27%) 1.3593\n",
            "84m 4s (- 225m 26s) (280000 27%) 1.3343\n",
            "84m 18s (- 224m 58s) (281000 27%) 1.3456\n",
            "84m 32s (- 224m 29s) (282000 27%) 1.3297\n",
            "84m 46s (- 224m 0s) (283000 27%) 1.3107\n",
            "85m 0s (- 223m 32s) (284000 27%) 1.3369\n",
            "85m 14s (- 223m 3s) (285000 27%) 1.3508\n",
            "85m 28s (- 222m 35s) (286000 27%) 1.3571\n",
            "85m 42s (- 222m 7s) (287000 27%) 1.3615\n",
            "85m 56s (- 221m 39s) (288000 27%) 1.3008\n",
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.5222947\n",
            "Rouge1 precision:\t 0.49494678\n",
            "Rouge1 recall:  \t 0.5616598\n",
            "Rouge2 fmeasure:\t 0.35628888\n",
            "Rouge2 precision:\t 0.3320282\n",
            "Rouge2 recall:  \t 0.39270967\n",
            "=====================================\n",
            "Epoch: 15/50\n",
            "86m 29s (- 221m 59s) (289000 28%) 1.2669\n",
            "86m 43s (- 221m 33s) (290000 28%) 1.3343\n",
            "86m 58s (- 221m 6s) (291000 28%) 1.2859\n",
            "87m 11s (- 220m 37s) (292000 28%) 1.2971\n",
            "87m 26s (- 220m 11s) (293000 28%) 1.3209\n",
            "87m 40s (- 219m 44s) (294000 28%) 1.2907\n",
            "87m 54s (- 219m 16s) (295000 28%) 1.3532\n",
            "88m 9s (- 218m 51s) (296000 28%) 1.3039\n",
            "88m 24s (- 218m 26s) (297000 28%) 1.3244\n",
            "88m 40s (- 218m 2s) (298000 28%) 1.2668\n",
            "88m 54s (- 217m 37s) (299000 29%) 1.2447\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 512\n",
        "\n",
        "bidirectional = False\n",
        "case = -1\n",
        "\n",
        "if case == 0: # LSTM & bi-LSTM\n",
        "    ust_lstm = True\n",
        "    encoder1 = EncoderLSTM(input_lang.n_words, hidden_size, bidirectional=bidirectional).to(device)\n",
        "    decoder1 = DecoderLSTM(hidden_size, output_lang.n_words).to(device)\n",
        "elif case == 1: # wrong\n",
        "    attention = True\n",
        "    encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "    decoder1 = Decoder(hidden_size, output_lang.n_words).to(device)\n",
        "elif case == 2: # transformer\n",
        "    useTransformer = True\n",
        "    \n",
        "    ntokens = input_lang.n_words # size of vocabulary\n",
        "    emsize = hidden_size  # embedding dimension\n",
        "    d_hid = 2048  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
        "    nlayers = 4  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
        "    nhead = 4  # number of heads in ``nn.MultiheadAttention``\n",
        "    dropout = 0.1  # dropout probability\n",
        "    encoder1 = Transformer(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "    decoder1 = Decoder(hidden_size, output_lang.n_words).to(device)\n",
        "elif case == 3: # improved attention\n",
        "    encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "    decoder1 = AttnDecoderRNN(hidden_size,\n",
        "                                  hidden_size,\n",
        "                                  output_lang.n_words,\n",
        "                                  15,\n",
        "                                  dropout_p=0.1).to(device)\n",
        "elif case == 4: # improvement\n",
        "    bidirectional = True\n",
        "    encoder1 = EncoderLSTM(input_lang.n_words, hidden_size, bidirectional=bidirectional).to(device)\n",
        "    decoder1 = AttnDecoderRNN(hidden_size,\n",
        "                                  hidden_size,\n",
        "                                  output_lang.n_words,\n",
        "                                  15,\n",
        "                                  dropout_p=0.1).to(device)\n",
        "else:\n",
        "    encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "    decoder1 = Decoder(hidden_size, output_lang.n_words).to(device)\n",
        "l = trainIters(encoder1, decoder1, 50, print_every=1000, learning_rate=6e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if isinstance(l[0], torch.Tensor): l = [i.cpu().detach().numpy() for i in l]\n",
        "plot(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc2y_EGQUT-c",
        "outputId": "95ce4243-39ab-4b85-e91d-36b5dd7f655c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> il dit qu il doit se rendre a vienne apres demain .\n",
            "= he says that he has to go to vienna the day after tomorrow .\n",
            "< he said that he would be a . . . <EOS>\n",
            "\n",
            "> j ai peur de ne pas etre d accord avec toi .\n",
            "= i m afraid i don t agree with you .\n",
            "< i m afraid i i t . . . . . <EOS>\n",
            "\n",
            "> il est taciturne .\n",
            "= he is a man of few words .\n",
            "< he is a . <EOS>\n",
            "\n",
            "> elle est photographe professionnelle .\n",
            "= she s a professional photographer .\n",
            "< she is a . . . <EOS>\n",
            "\n",
            "> il est deja la .\n",
            "= he is already here .\n",
            "< he is very . . <EOS>\n",
            "\n",
            "> ils sont frere et s ur .\n",
            "= they re brother and sister .\n",
            "< they re both in the . . <EOS>\n",
            "\n",
            "> il est probable qu il gagne la partie .\n",
            "= he is likely to win the game .\n",
            "< he is always to to . . . <EOS>\n",
            "\n",
            "> tu es adorable .\n",
            "= you re adorable .\n",
            "< you re nuts . <EOS>\n",
            "\n",
            "> il se planque .\n",
            "= he s gone into hiding .\n",
            "< he is a . <EOS>\n",
            "\n",
            "> je suis fatigue de tous ses mensonges .\n",
            "= i m fed up with all their lies .\n",
            "< i m sure of the . . . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcJQDHJ2-96c",
        "outputId": "d1c261a2-dfe0-4a5f-e96e-59169c50e734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.4360251\n",
            "Rouge1 precision:\t 0.43233687\n",
            "Rouge1 recall:  \t 0.4500943\n",
            "Rouge2 fmeasure:\t 0.25591362\n",
            "Rouge2 precision:\t 0.25067902\n",
            "Rouge2 recall:  \t 0.27076897\n",
            "=====================================\n"
          ]
        }
      ],
      "source": [
        "input,gt,predict,score = test(encoder1, decoder1, train_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F81S9tbV6vtL",
        "outputId": "2706a9fc-6a7e-4659-e89f-2fa832ce8955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.43335107\n",
            "Rouge1 precision:\t 0.4290823\n",
            "Rouge1 recall:  \t 0.4479097\n",
            "Rouge2 fmeasure:\t 0.25470656\n",
            "Rouge2 precision:\t 0.24900529\n",
            "Rouge2 recall:  \t 0.26992095\n",
            "=====================================\n"
          ]
        }
      ],
      "source": [
        "input,gt,predict,score = test(encoder1, decoder1, test_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \\\\\n",
            "seq2seq (original) & 84.83 & 78.36 & 92.87 & 76.72 & 69.62 & 86.07 \\\\\n",
            "seq2seq (LSTM) & 84.18 & 78.21 & 91.57 & 76.09 & 69.49 & 84.75 \\\\\n",
            "seq2seq (bi-LSTM) & 85.58 & 79.35 & 93.29 & 78.13 & 71.2 & 87.2 \\\\\n",
            "seq2seq (Attention) & 88.18 & 81.48 & 96.35 & 83.14 & 75.54 & 92.92 \\\\\n",
            "seq2seq (Transformer Encoder) & 52.69 & 51.4 & 56.19 & 34.85 & 33.38 & 38.39 \\\\\n",
            "seq2seq (BI-LSTM + Attention) & 84.77 & 78.55 & 92.38 & 77.18 & 70.34 & 85.99 \\\\\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "with open(\"train_log.txt\", \"r\") as txt:\n",
        "    lines = txt.readlines()\n",
        "    buffer = []\n",
        "    cat = \"\"\n",
        "    for line in lines:\n",
        "        if line.startswith(\"seq2seq\"):\n",
        "            cat += \" \\\\\\\\\"\n",
        "            buffer.append(cat)\n",
        "            cat = line.split(\"\\n\")[0]\n",
        "        else:\n",
        "            if len(re.split(r':\\s*', line.strip())) > 1:\n",
        "                cat += \" & \" + str(round(float(re.split(r':\\s*', line.strip())[1])*100, 2))\n",
        "    buffer.append(cat+\" \\\\\\\\\")\n",
        "    for i in buffer: print(i)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "seq2seq (bi-LSTM) & 0.591 & 69.42\\% \\\\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
